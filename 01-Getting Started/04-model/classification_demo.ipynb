{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本分类实例"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1 导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pika/App/miniconda3/envs/transformers/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>距离川沙公路较近,但是公交指示不对,如果是\"蔡陆线\"的话,会非常麻烦.建议用别的路线.房间较...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>商务大床房，房间很大，床有2M宽，整体感觉经济实惠不错!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>早餐太差，无论去多少人，那边也不加食品的。酒店应该重视一下这个问题了。房间本身很好。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>宾馆在小街道上，不大好找，但还好北京热心同胞很多~宾馆设施跟介绍的差不多，房间很小，确实挺小...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>CBD中心,周围没什么店铺,说5星有点勉强.不知道为什么卫生间没有电吹风</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                             review\n",
       "0      1  距离川沙公路较近,但是公交指示不对,如果是\"蔡陆线\"的话,会非常麻烦.建议用别的路线.房间较...\n",
       "1      1                       商务大床房，房间很大，床有2M宽，整体感觉经济实惠不错!\n",
       "2      1         早餐太差，无论去多少人，那边也不加食品的。酒店应该重视一下这个问题了。房间本身很好。\n",
       "3      1  宾馆在小街道上，不大好找，但还好北京热心同胞很多~宾馆设施跟介绍的差不多，房间很小，确实挺小...\n",
       "4      1               CBD中心,周围没什么店铺,说5星有点勉强.不知道为什么卫生间没有电吹风"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"./ChnSentiCorp_htl_all.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7765, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.dropna()\n",
    "data.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3 创建Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.data = pd.read_csv(\"./ChnSentiCorp_htl_all.csv\")\n",
    "        self.data = self.data.dropna()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data.iloc[index][\"review\"], self.data.iloc[index][\"label\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('距离川沙公路较近,但是公交指示不对,如果是\"蔡陆线\"的话,会非常麻烦.建议用别的路线.房间较为简单.', 1)\n",
      "('商务大床房，房间很大，床有2M宽，整体感觉经济实惠不错!', 1)\n",
      "('早餐太差，无论去多少人，那边也不加食品的。酒店应该重视一下这个问题了。房间本身很好。', 1)\n",
      "('宾馆在小街道上，不大好找，但还好北京热心同胞很多~宾馆设施跟介绍的差不多，房间很小，确实挺小，但加上低价位因素，还是无超所值的；环境不错，就在小胡同内，安静整洁，暖气好足-_-||。。。呵还有一大优势就是从宾馆出发，步行不到十分钟就可以到梅兰芳故居等等，京味小胡同，北海距离好近呢。总之，不错。推荐给节约消费的自助游朋友~比较划算，附近特色小吃很多~', 1)\n",
      "('CBD中心,周围没什么店铺,说5星有点勉强.不知道为什么卫生间没有电吹风', 1)\n"
     ]
    }
   ],
   "source": [
    "dataset = MyDataset()\n",
    "for i in range(5):\n",
    "    print(dataset[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4 划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6989, 776)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "trainset, validset = random_split(dataset, lengths=[0.9, 0.1])\n",
    "len(trainset), len(validset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('酒店看起来设施都比较陈旧，房间里居然没有中控，非常不方便。热水到还不错。前台态度一般.', 0)\n",
      "('酒店员工都很亲切随时报以笑容，但是酒店不算新了，早餐品种单一，每天都一样，晚上的自助晚餐菜品也一般，信价比不高哦，还不如打车去滨海路的餐厅吃，好吃又便宜。酒店spa,健身中心，大堂都非常的普通，游泳池也不算大，不过酒店对面就是私人海滩，可以在海滩边消磨一天，还是不错的。', 1)\n",
      "('天清岛度假酒店，自然风景很好，优于天元度假村。位置偏僻，适合团队和自驾游。设施一般，相比天元，差距很大。早餐足够丰盛，质量也很好。酒店自制的面包和小点心非常好吃，尤其推荐枣泥蛋糕。早餐培根特别好，广受欢迎，水果的品质也不错。中餐厅的正餐就没什么特色了，味道一般，好在价格没有贵得离谱。开元的中餐厅味道确实很好，除了鱼头，我们对那个过油四季豆印象很深，味道独特，很好吃。关于房间：天清岛酒店共9个楼。1-8号为一个整体，呈蛇形排列，共5层，1层是停车库，2-5层客房。大堂位于中间，左侧是1、2号，右侧依次是3、4、5、6、7、8号。9号楼在岛上是看不到的，位于停车坪和草坪的下面，只有2层，紧邻湖水，要在水上或山对面才能看见正面。1-9号所有房间都是景观房，有观景阳台。房间景观首推9号楼，阳台下面就是湖水，远处是岛屿和层峦，视线开阔无遮拦，有水墨画的写意韵味。“缺点”是有点单调。这也是天清岛和开元最大的差别所在。开元胜在装修和园林景观，精致豪华且丰富多彩，天清岛胜在天水合一的自然景观，没有人工雕饰，但略显单调。选择哪里，要看个人喜好。9号楼的阳台上有茶几和太阳椅，坐在外面看风景、扮风景时，别忘了带个望远镜。我们订到一天豪华房，第二天在房间一直耗到中午才离开。冲杯咖啡，在阳台上看湖面驶过的游船和快艇，再望望远山，非常悠闲惬意。4、5号楼的景观与9号不相上下，面向辽阔的湖水，视线最为开阔，且站的高看的远。有人说隔条马路才是湖面，我觉得这不是问题，因为楼前有酒店的园林绿化，反而丰富了景观效果。这里特别提醒，如果不是一定要睡大床，酒店2张1米2宽单人床的标准间最为实惠。因为湖景大床房全部安排在景观不佳的6号楼，房间大小、设施没有区别，价格却高于其他楼的湖景标间，不划算。之后依次是7、3、6、8号。所谓景观不佳，就是视角逊色、不够开阔。我们第二晚换到6号楼大床房，正对一个小水湾。看出去青山碧水，也很美，但在千岛湖，这样的景观实在不足为奇。虽然站到阳台上也能看到旁边的辽阔水面，但相对4、5号楼，就逊色许多。我们同行的人住在3号楼，因为处在折线中，要站在阳台上左望，才能看到主湖区。7号楼就好些，我没机会进去，看角度优于其他。订房时除了景观还要考虑位置，8号楼因为在酒店最远端，视线也一般，所以游客都不愿意问津。但8号也有优势，旁边就是易主题文化公园，去那里参观很方便。1、2号楼就不是我们散客光顾的了，都是套房和复式，我看景观也不大好。还要提醒一点，9号楼的玻璃淋浴间是瓷砖地面，踩上去冰凉，在秋冬季可不怎么舒服。备用的橡胶脚垫脏脏的，最好不用。', 1)\n",
      "('性价比还可以,装修设备都挺不错~就是有些房间没有窗户~闷~还有就是有乱按门铃的~', 1)\n",
      "('应该算是阳朔最好的酒店了，在西街头上，距离汽车站又近，地段没的说了。而且要过一个桥在到酒店里面，闹重取静，也是一大亮点。不过阳朔毕竟是小地方，酒店房间设施应该达不到四星级的标准。', 1)\n",
      "('总体来说还是不错的。说是酒店，其实更像是个公寓，一方面楼里有好几层是办公用的，另一方面房间的配备也像。我们订的是豪华大床房，先说优点：入住和退房都挺快的，商务中心可以订车票，房间不小挺干净的，有卫星电视。缺点也有几个：一是离西湖有点远(20-30分钟行走)，二是周围吃饭的地方太少，只有三家，早餐得走15分钟到延安路才有(不过好吃又不贵，人很多)，三是浴室有点小，没有单独的沐浴房。宾馆反馈2007年11月5日：首先感谢您对我酒店提出宝贵的评价，在这里我们对您提出的一些问题将做一些相应的解释：1、酒店离西湖的总路程为一站路的公交车程。2、从我酒店出门（庆春路）往东方向步行大约2分钟，就有许多饭店包括早餐店，价格实惠且品种丰富，这条街是杭州餐饮比较集中的一条街，建议您下次入住时可直接咨询前台，我们将热忱为您服务。', 1)\n",
      "('2008年7月29日入住三人房一晚，虽说是套房，但是只有一个空调出风口，一间房冻死，一间房还行。淋浴房的水是往外流的，洗个澡卫生间湿了一半了。花洒有很大问题出水极小，要求更换了，还是一样的毛病，根本没法洗头。到半夜大概11点多的时候，我的房间居然会停电，我只开了空调和电视，一人在洗澡而已。还有就是早餐基本没什么吃的。', 0)\n",
      "('环境好，在公园内，300元的价格也不贵。用餐一般，还是出去吃，在聚福林吃了两餐，真好。', 1)\n",
      "('我也是通过比较携程的用户评论选择海景的，是第一次住。总体感觉很好，房间硬件一般但很干净，每天有水果、糖果赠送，第一天还送玩偶，小孩子很喜欢。特别要提的是酒店服务很周到，餐饮和包车的价格也很公道，我们在酒店吃了两次晚餐都非常的满意，人均消费在30元左右（含酒水的哦）。同去的朋友都很满意，下次去威海还会住海景的。补充点评2008年7月29日：这里的工作人员各个彬彬有礼，作为一家国有企业实在难能可贵。例如，我早餐后刚在大堂的沙发坐下，服务员立即给我送上茶水和毛巾；我们在二楼公共休息区打牌，服务员见天色将黑，就主动把灯打开......', 1)\n",
      "('这个酒店是当地最好的了，就是这样一个中小城市酒店价格不便宜，三种价格的房间其实区别不大，最便宜的就是没有地毯，木板的，最贵的只是送了两包咖啡，晚上偶尔有送花，所以自己来玩的可以住最便宜的。这个城市也就是这个酒店了。', 1)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(trainset[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5 创建Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"hfl/rbt3\")\n",
    "\n",
    "\n",
    "# 没在dataset里处理：tokenizer在batch处理是最快的\n",
    "def collate_func(batch):\n",
    "    texts, labels = [], []\n",
    "    for item in batch:\n",
    "        texts.append(item[0])\n",
    "        labels.append(item[1])\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        max_length=128,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )  # dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
    "\n",
    "    inputs[\"labels\"] = torch.tensor(labels)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=32, shuffle=True, collate_fn=collate_func)\n",
    "validloader = DataLoader(\n",
    "    validset, batch_size=64, shuffle=False, collate_fn=collate_func\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 3302, 1218,  ...,    0,    0,    0],\n",
       "        [ 101, 1914, 2399,  ...,    0,    0,    0],\n",
       "        [ 101, 1762, 6935,  ...,  511,  852,  102],\n",
       "        ...,\n",
       "        [ 101, 3680, 3613,  ..., 1184, 6842,  102],\n",
       "        [ 101, 2791, 7313,  ...,    0,    0,    0],\n",
       "        [ 101, 2523, 3173,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,\n",
       "        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(enumerate(validloader))[1]  # 这个时候调用collate_func"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step6 创建模型及优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/rbt3 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"hfl/rbt3\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step7 训练与验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    model.eval()\n",
    "    acc_num = 0\n",
    "    with torch.inference_mode():\n",
    "        for batch in validloader:\n",
    "            if torch.cuda.is_available():\n",
    "                batch = {k: v.cuda() for k, v in batch.items()}\n",
    "            output = model(**batch)\n",
    "            pred = torch.argmax(output.logits, dim=-1)\n",
    "            acc_num += (pred.long() == batch[\"labels\"].long()).float().sum()\n",
    "    return acc_num / len(validset)\n",
    "\n",
    "\n",
    "def train(epoch=3, log_step=100):\n",
    "    global_step = 0\n",
    "    for ep in range(epoch):\n",
    "        model.train()\n",
    "        for batch in trainloader:\n",
    "            if torch.cuda.is_available():\n",
    "                batch = {k: v.cuda() for k, v in batch.items()}\n",
    "            optimizer.zero_grad()\n",
    "            output = model(**batch)\n",
    "            output.loss.backward()\n",
    "            optimizer.step()\n",
    "            if global_step % log_step == 0:\n",
    "                print(\n",
    "                    f\"ep: {ep}, global_step: {global_step}, loss: {output.loss.item()}\"\n",
    "                )\n",
    "            global_step += 1\n",
    "        acc = evaluate()\n",
    "        print(f\"ep: {ep}, acc: {acc}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step8 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 0, global_step: 0, loss: 0.7017712593078613\n",
      "ep: 0, global_step: 100, loss: 0.24556273221969604\n",
      "ep: 0, global_step: 200, loss: 0.3049444258213043\n",
      "ep: 0, acc: 0.8981958627700806\n",
      "ep: 1, global_step: 300, loss: 0.19269973039627075\n",
      "ep: 1, global_step: 400, loss: 0.21679648756980896\n",
      "ep: 1, acc: 0.9033504724502563\n",
      "ep: 2, global_step: 500, loss: 0.15411913394927979\n",
      "ep: 2, global_step: 600, loss: 0.057508256286382675\n",
      "ep: 2, acc: 0.905927836894989\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step9 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入：我觉得这家酒店不错，饭很好吃！\n",
      "模型预测结果:好评！\n"
     ]
    }
   ],
   "source": [
    "sen = \"我觉得这家酒店不错，饭很好吃！\"\n",
    "id2_label = {0: \"差评！\", 1: \"好评！\"}\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    inputs = tokenizer(sen, return_tensors=\"pt\")\n",
    "    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "    logits = model(**inputs).logits\n",
    "    pred = torch.argmax(logits, dim=-1)\n",
    "    print(f\"输入：{sen}\\n模型预测结果:{id2_label.get(pred.item())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model.config.id2label = id2_label\n",
    "pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '好评！', 'score': 0.9969767332077026}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(sen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
