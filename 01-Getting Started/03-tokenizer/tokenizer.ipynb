{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基础组件之Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer基本使用方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pika/App/miniconda3/envs/transformers/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = \"弱小的我也有大梦想!\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载与保存: `from_pretrained` / `save_pretrained`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='uer/roberta-base-finetuned-dianping-chinese', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从HuggingFace加载，输入模型名称，即可加载对于的分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"uer/roberta-base-finetuned-dianping-chinese\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./roberta_tokenizer/tokenizer_config.json',\n",
       " './roberta_tokenizer/special_tokens_map.json',\n",
       " './roberta_tokenizer/vocab.txt',\n",
       " './roberta_tokenizer/added_tokens.json',\n",
       " './roberta_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer 保存到本地\n",
    "tokenizer.save_pretrained(\"./roberta_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='./roberta_tokenizer/', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从本地加载tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./roberta_tokenizer/\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, '[PAD]')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id, tokenizer.pad_token"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 句子分词: `tokenize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['弱', '小', '的', '我', '也', '有', '大', '梦', '想', '!']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(sen)\n",
    "tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查看词典: `vocab`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'##吸': 14486,\n",
       " 'pizza': 10315,\n",
       " '##倶': 14021,\n",
       " '##徴': 15603,\n",
       " 'ng': 8885,\n",
       " '##懑': 15806,\n",
       " '##蓉': 18957,\n",
       " '##奧': 15010,\n",
       " '##氏': 16751,\n",
       " '##薦': 19013,\n",
       " '##et': 8418,\n",
       " '滙': 4002,\n",
       " '純': 5155,\n",
       " '∙': 379,\n",
       " '##疡': 17607,\n",
       " '##芝': 18755,\n",
       " '##ory': 9428,\n",
       " '许': 6387,\n",
       " '瀏': 4104,\n",
       " 'になる': 9322,\n",
       " '##\\u2028': 13502,\n",
       " '##湧': 17020,\n",
       " 'de': 8363,\n",
       " '##宕': 15190,\n",
       " '##荞': 18836,\n",
       " '##骇': 20794,\n",
       " '##卻': 14377,\n",
       " '##擲': 16153,\n",
       " '##盧': 17735,\n",
       " '##退': 19899,\n",
       " '265': 8689,\n",
       " '##钾': 20242,\n",
       " '##偿': 14042,\n",
       " '菅': 5822,\n",
       " '鄺': 6976,\n",
       " '##簇': 18134,\n",
       " '哐': 1513,\n",
       " '唷': 1550,\n",
       " '蜈': 6048,\n",
       " '娓': 2022,\n",
       " '每': 3680,\n",
       " '900': 8567,\n",
       " '藓': 5968,\n",
       " '脂': 5544,\n",
       " '瓶': 4486,\n",
       " '##脚': 18615,\n",
       " '驭': 7717,\n",
       " '##『': 13656,\n",
       " '##智': 16312,\n",
       " '缎': 5352,\n",
       " '悼': 2656,\n",
       " '郵': 6960,\n",
       " '##殃': 16707,\n",
       " 'cafe': 8377,\n",
       " '##豚': 19552,\n",
       " '蛇': 6026,\n",
       " 'valley': 11994,\n",
       " '蕻': 5944,\n",
       " '増': 1868,\n",
       " '辮': 6799,\n",
       " '裔': 6167,\n",
       " '##觐': 19290,\n",
       " '##猴': 17404,\n",
       " '[unused86]': 86,\n",
       " '弁': 2459,\n",
       " '##茂': 18801,\n",
       " '訕': 6247,\n",
       " '3a': 10667,\n",
       " '##棱': 16539,\n",
       " '揆': 2986,\n",
       " '##玲': 17443,\n",
       " '州': 2336,\n",
       " '##2d': 12675,\n",
       " '##账': 19629,\n",
       " '##瓢': 17536,\n",
       " '##踞': 19732,\n",
       " '叨': 1370,\n",
       " '劳': 1227,\n",
       " '##ple': 10383,\n",
       " '##qi': 11451,\n",
       " '鳶': 7856,\n",
       " '##倬': 14019,\n",
       " '谐': 6455,\n",
       " '##觀': 19280,\n",
       " '##閾': 20349,\n",
       " '尽': 2226,\n",
       " '##😎': 21127,\n",
       " '1929': 9792,\n",
       " '##おります': 9797,\n",
       " '傀': 986,\n",
       " '堵': 1843,\n",
       " 'ssh': 11678,\n",
       " '詮': 6280,\n",
       " 'golden': 12746,\n",
       " '鴕': 7858,\n",
       " 'si': 10883,\n",
       " '凋': 1118,\n",
       " '撈': 3051,\n",
       " '麽': 7939,\n",
       " 'fks': 10044,\n",
       " '焊': 4184,\n",
       " '瑪': 4454,\n",
       " 'insee': 11513,\n",
       " 'サイト': 11572,\n",
       " '##懿': 15817,\n",
       " '##е': 13406,\n",
       " 'なと': 9744,\n",
       " '崴': 2311,\n",
       " '##佥': 13933,\n",
       " '##馈': 20725,\n",
       " '##棋': 16527,\n",
       " '##ß': 13361,\n",
       " 'し': 547,\n",
       " 'systems': 12451,\n",
       " '##衍': 19179,\n",
       " '##ndy': 11373,\n",
       " '##捲': 16004,\n",
       " '知': 4761,\n",
       " 'android': 8254,\n",
       " '盆': 4658,\n",
       " '##xure': 12622,\n",
       " '蒟': 5888,\n",
       " 'ed': 9599,\n",
       " '屎': 2241,\n",
       " '##未': 16370,\n",
       " '吳': 1425,\n",
       " '禅': 4883,\n",
       " 'fc2': 11362,\n",
       " '##贖': 19619,\n",
       " '##お': 8345,\n",
       " '##闷': 20372,\n",
       " '兄': 1040,\n",
       " '張': 2484,\n",
       " '##46': 9340,\n",
       " '##紊': 18207,\n",
       " '艋': 5674,\n",
       " '1916': 10772,\n",
       " '堰': 1840,\n",
       " '##tis': 10827,\n",
       " '##衩': 19192,\n",
       " '##引': 15528,\n",
       " '##览': 19286,\n",
       " '疼': 4563,\n",
       " '##銑': 20125,\n",
       " '责': 6569,\n",
       " '##匹': 14333,\n",
       " '閥': 7285,\n",
       " '425': 11993,\n",
       " '##隆': 20441,\n",
       " '绘': 5313,\n",
       " '347': 12936,\n",
       " 'ㆍ': 666,\n",
       " '##℃': 8320,\n",
       " '枇': 3355,\n",
       " '##pmlast': 12138,\n",
       " '##儕': 14085,\n",
       " '##篩': 18129,\n",
       " 'cam': 12722,\n",
       " '柔': 3382,\n",
       " '##吏': 14458,\n",
       " '护': 2844,\n",
       " 'want': 12733,\n",
       " '##盱': 17739,\n",
       " '涯': 3889,\n",
       " 'news': 8501,\n",
       " '抓': 2831,\n",
       " '30': 8114,\n",
       " '##а': 11318,\n",
       " '唸': 1551,\n",
       " 'nginx': 12576,\n",
       " '##莖': 18861,\n",
       " '伕': 829,\n",
       " '槎': 3542,\n",
       " '##nsis': 12263,\n",
       " '啲': 1576,\n",
       " '##梢': 16513,\n",
       " '##bot': 10119,\n",
       " '##劲': 14283,\n",
       " '##泄': 16843,\n",
       " '##祟': 17926,\n",
       " '嗅': 1618,\n",
       " '蜀': 6043,\n",
       " '##徽': 15608,\n",
       " '褶': 6195,\n",
       " 'pci': 10773,\n",
       " '##闯': 20367,\n",
       " 'ᅣ': 306,\n",
       " '##read': 10314,\n",
       " '妮': 1984,\n",
       " 'b1': 9338,\n",
       " '##熟': 17282,\n",
       " '##駐': 20745,\n",
       " '猷': 4349,\n",
       " 'red': 9276,\n",
       " '##阅': 20382,\n",
       " '殉': 3653,\n",
       " '拟': 2877,\n",
       " '攣': 3112,\n",
       " '##欠': 16669,\n",
       " '餌': 7622,\n",
       " '##態': 15763,\n",
       " '[unused14]': 14,\n",
       " '厢': 1334,\n",
       " '##鳳': 20911,\n",
       " '隠': 7398,\n",
       " '##侈': 13947,\n",
       " '##访': 19450,\n",
       " '楼': 3517,\n",
       " '９': 8037,\n",
       " '敌': 3127,\n",
       " '##bor': 12268,\n",
       " 'joy': 12668,\n",
       " '##馭': 20737,\n",
       " '口': 1366,\n",
       " '##ncy': 10518,\n",
       " '##杏': 16388,\n",
       " '峇': 2280,\n",
       " '##褓': 19245,\n",
       " '演': 4028,\n",
       " '汲': 3744,\n",
       " '椰': 3496,\n",
       " '牌': 4277,\n",
       " '！': 8013,\n",
       " '##vis': 11233,\n",
       " 'area': 13224,\n",
       " '4': 125,\n",
       " '##酩': 20047,\n",
       " 'chang': 11680,\n",
       " '##葫': 18929,\n",
       " '119': 9031,\n",
       " '##满': 17064,\n",
       " '伤': 839,\n",
       " '##纱': 18342,\n",
       " '蠹': 6115,\n",
       " '##聞': 18529,\n",
       " '##堇': 14889,\n",
       " '窺': 4983,\n",
       " '允': 1038,\n",
       " '著': 5865,\n",
       " 'dram': 10664,\n",
       " '590': 11210,\n",
       " '上': 677,\n",
       " 'jack': 9850,\n",
       " '##ve': 8519,\n",
       " '##勧': 14307,\n",
       " '##將': 15257,\n",
       " '##析': 16415,\n",
       " '合': 1394,\n",
       " '殇': 3652,\n",
       " '##36': 9159,\n",
       " '空': 4958,\n",
       " '##接': 16027,\n",
       " 'beauty': 9445,\n",
       " '潔': 4049,\n",
       " '羿': 5418,\n",
       " 'm5': 12802,\n",
       " '##毒': 16738,\n",
       " 'suite': 11420,\n",
       " 'っている': 11859,\n",
       " '386': 12303,\n",
       " '##箇': 18100,\n",
       " '##氪': 16768,\n",
       " '##紓': 18211,\n",
       " '##匾': 14336,\n",
       " '##ت': 13430,\n",
       " '劣': 1219,\n",
       " '##嗲': 14697,\n",
       " '鑫': 7144,\n",
       " 'オーフン5': 11810,\n",
       " '1894': 12221,\n",
       " '﹕': 8004,\n",
       " 'mix': 9678,\n",
       " '##gt': 12429,\n",
       " '##劾': 14288,\n",
       " '##祺': 17935,\n",
       " '##蠣': 19169,\n",
       " 'vive': 10167,\n",
       " '##號': 19055,\n",
       " '擢': 3091,\n",
       " '謗': 6339,\n",
       " '迴': 6836,\n",
       " 'dollars': 9448,\n",
       " '拉': 2861,\n",
       " '累': 5168,\n",
       " 'yang': 12086,\n",
       " '5d': 10406,\n",
       " '笔': 5011,\n",
       " '##ц': 13421,\n",
       " '##児': 14106,\n",
       " '荆': 5769,\n",
       " '314': 11725,\n",
       " '##剿': 14261,\n",
       " '##睜': 17772,\n",
       " '争': 751,\n",
       " 'january': 9768,\n",
       " '拼': 2894,\n",
       " '布': 2357,\n",
       " '購': 6554,\n",
       " '00': 8136,\n",
       " '##橫': 16642,\n",
       " '##變': 19422,\n",
       " '侵': 909,\n",
       " '##貶': 19581,\n",
       " 'maker': 11389,\n",
       " '##踪': 19736,\n",
       " '##铅': 20249,\n",
       " '##龈': 21036,\n",
       " '##愁': 15744,\n",
       " '##揆': 16043,\n",
       " '##慰': 15777,\n",
       " '葯': 5875,\n",
       " '鶴': 7874,\n",
       " '##鍍': 20160,\n",
       " '##雨': 20490,\n",
       " '##鑼': 20204,\n",
       " '##ube': 10957,\n",
       " '##52': 9364,\n",
       " '##痊': 17628,\n",
       " '##杠': 16396,\n",
       " '鹽': 7921,\n",
       " '##皎': 17700,\n",
       " '##过': 19871,\n",
       " '续': 5330,\n",
       " '##unch': 11294,\n",
       " '篓': 5066,\n",
       " '##ann': 12464,\n",
       " '##☞': 13621,\n",
       " '##璇': 17519,\n",
       " '丑': 682,\n",
       " '秸': 4918,\n",
       " '裱': 6177,\n",
       " '##愕': 15750,\n",
       " '瀧': 4113,\n",
       " '徠': 2538,\n",
       " '##ヘ': 13693,\n",
       " '饞': 7644,\n",
       " '##彥': 15560,\n",
       " ',': 117,\n",
       " '##7': 8161,\n",
       " '##⑩': 13565,\n",
       " '惦': 2671,\n",
       " 'ね': 560,\n",
       " '##盯': 17738,\n",
       " '##品': 14558,\n",
       " 'kicstart2': 12614,\n",
       " '##殼': 16727,\n",
       " '键': 7241,\n",
       " '薪': 5959,\n",
       " '##煒': 17261,\n",
       " '##磅': 17886,\n",
       " '##蝎': 19127,\n",
       " '虔': 5992,\n",
       " 'lg': 8589,\n",
       " '嘉': 1649,\n",
       " '懈': 2745,\n",
       " '##ータ': 12219,\n",
       " '］': 8047,\n",
       " '##竞': 18050,\n",
       " '##2007': 10604,\n",
       " '##og': 10800,\n",
       " '##聚': 18528,\n",
       " 'ac': 9226,\n",
       " 'mall': 9628,\n",
       " '經': 5195,\n",
       " '540': 10972,\n",
       " '##栃': 16457,\n",
       " '##遴': 19961,\n",
       " '##鍋': 20159,\n",
       " '##﹡': 21068,\n",
       " '谍': 6452,\n",
       " '##察': 15232,\n",
       " '##与': 13737,\n",
       " '9985': 12839,\n",
       " '##麝': 20985,\n",
       " '袜': 6154,\n",
       " '兽': 1077,\n",
       " 'g20': 9789,\n",
       " '錶': 7100,\n",
       " 'x': 166,\n",
       " '猝': 4340,\n",
       " '阪': 7341,\n",
       " '##决': 14161,\n",
       " '啱': 1575,\n",
       " '滿': 4021,\n",
       " 'bt': 8364,\n",
       " 'tiffany': 11509,\n",
       " '##滞': 17062,\n",
       " '##ᄀ': 13454,\n",
       " 'banner': 13256,\n",
       " '顺': 7556,\n",
       " 'ก': 276,\n",
       " '舖': 5655,\n",
       " '##琳': 17489,\n",
       " '##蝨': 19134,\n",
       " '2mm': 12287,\n",
       " '##得': 15590,\n",
       " 'george': 9897,\n",
       " '缽': 5376,\n",
       " '433': 13131,\n",
       " '俚': 923,\n",
       " '595': 13302,\n",
       " '##群': 18465,\n",
       " '更': 3291,\n",
       " '##佻': 13941,\n",
       " '##锢': 20292,\n",
       " '##ために': 12260,\n",
       " '##渥': 17001,\n",
       " '##ien': 12023,\n",
       " '##2009': 9948,\n",
       " '##匐': 14320,\n",
       " '##較': 19790,\n",
       " '##颱': 20650,\n",
       " 'com™': 10921,\n",
       " '##卿': 14378,\n",
       " '##鈕': 20103,\n",
       " '##hai': 11828,\n",
       " '##晞': 16300,\n",
       " '涤': 3882,\n",
       " '褂': 6184,\n",
       " 'ᅡ': 304,\n",
       " '##浒': 16905,\n",
       " '##淅': 16954,\n",
       " '恒': 2608,\n",
       " '鑽': 7148,\n",
       " '沫': 3773,\n",
       " '1978': 8774,\n",
       " 'rom': 8891,\n",
       " 'thunder': 11322,\n",
       " '##挺': 15980,\n",
       " '##蚵': 19080,\n",
       " '##许': 19444,\n",
       " '##獒': 17413,\n",
       " '辻': 6806,\n",
       " '纔': 5269,\n",
       " '柚': 3384,\n",
       " 'ｰ': 8087,\n",
       " 'ky': 11096,\n",
       " '##電': 20499,\n",
       " 'film': 11149,\n",
       " '##哀': 14557,\n",
       " '##ial': 9501,\n",
       " '##潛': 17108,\n",
       " '灼': 4133,\n",
       " '５０': 12869,\n",
       " '##夔': 14967,\n",
       " '##镌': 20311,\n",
       " 'urn': 11584,\n",
       " '醯': 7018,\n",
       " '║': 439,\n",
       " '##vc': 12077,\n",
       " '严': 698,\n",
       " '坐': 1777,\n",
       " '怡': 2592,\n",
       " 'day': 8542,\n",
       " 'bloomberg': 10313,\n",
       " 'can': 9109,\n",
       " '##タ': 11011,\n",
       " '##剑': 14244,\n",
       " '##钏': 20212,\n",
       " '##養': 20678,\n",
       " 'rn': 11998,\n",
       " '##茁': 18800,\n",
       " '##矾': 17827,\n",
       " '##鸿': 20953,\n",
       " 'belle': 13058,\n",
       " 'rich': 13210,\n",
       " '##迄': 19869,\n",
       " '##兰': 14122,\n",
       " '何': 862,\n",
       " '✔': 497,\n",
       " '##疮': 17612,\n",
       " '粘': 5111,\n",
       " '##岖': 15322,\n",
       " '##ます': 8643,\n",
       " '##启': 14480,\n",
       " 'ᅦ': 308,\n",
       " '蝕': 6071,\n",
       " '霏': 7454,\n",
       " 'せます': 10046,\n",
       " '##佘': 13921,\n",
       " '獭': 4361,\n",
       " '岩': 2272,\n",
       " 'space': 9634,\n",
       " '佼': 885,\n",
       " '##✖': 13635,\n",
       " '##筲': 18094,\n",
       " '房': 2791,\n",
       " '遴': 6904,\n",
       " '##≤': 13545,\n",
       " '##粑': 18164,\n",
       " 'men': 11305,\n",
       " '悚': 2639,\n",
       " '##滋': 17053,\n",
       " '##螢': 19143,\n",
       " '蓋': 5901,\n",
       " '汝': 3734,\n",
       " 'kate': 11058,\n",
       " '1986': 8629,\n",
       " '##莠': 18864,\n",
       " '30g': 12008,\n",
       " '##琮': 17486,\n",
       " '樺': 3573,\n",
       " '##เ': 13450,\n",
       " 'bay': 10251,\n",
       " '##ssion': 12726,\n",
       " '##gs': 9726,\n",
       " '##なく': 10395,\n",
       " '##輝': 19797,\n",
       " '囤': 1732,\n",
       " '##复': 14965,\n",
       " 'office': 8628,\n",
       " '阻': 7349,\n",
       " '##ford': 10283,\n",
       " '##獰': 17419,\n",
       " '痿': 4594,\n",
       " 'د': 265,\n",
       " '539': 12995,\n",
       " '##闰': 20368,\n",
       " '##色': 18739,\n",
       " '隋': 7387,\n",
       " '##芪': 18760,\n",
       " '##諒': 19372,\n",
       " '纍': 5266,\n",
       " '##掉': 16014,\n",
       " '陛': 7363,\n",
       " '駝': 7692,\n",
       " '鼾': 7966,\n",
       " 'some': 13048,\n",
       " '##2': 8144,\n",
       " '蕨': 5938,\n",
       " '##care': 11014,\n",
       " '##mbps': 12399,\n",
       " '##夥': 14976,\n",
       " '##椽': 16555,\n",
       " '##沭': 16831,\n",
       " '##存': 15157,\n",
       " '膾': 5616,\n",
       " '##溥': 17038,\n",
       " '##宛': 15195,\n",
       " '茼': 5766,\n",
       " '##蠟': 19166,\n",
       " '罕': 5383,\n",
       " '鹰': 7916,\n",
       " '呎': 1442,\n",
       " '##鵑': 20921,\n",
       " '鍛': 7104,\n",
       " '磅': 4829,\n",
       " '瓣': 4480,\n",
       " 'fpga': 9724,\n",
       " '##凶': 14193,\n",
       " '##墜': 14928,\n",
       " '##晗': 16297,\n",
       " '##焉': 17240,\n",
       " 'urban': 13101,\n",
       " '##泵': 16865,\n",
       " '茫': 5755,\n",
       " '晕': 3238,\n",
       " 'august': 9217,\n",
       " 'ie': 8469,\n",
       " 'unity': 11897,\n",
       " '厲': 1341,\n",
       " '惡': 2670,\n",
       " 'ト': 613,\n",
       " '曖': 3281,\n",
       " '羸': 5415,\n",
       " '閱': 7288,\n",
       " 'paypal': 8657,\n",
       " '##袅': 19205,\n",
       " '[unused49]': 49,\n",
       " '26': 8153,\n",
       " '##依': 13955,\n",
       " '##嫔': 15125,\n",
       " '1970': 8464,\n",
       " '啊': 1557,\n",
       " 'block': 10188,\n",
       " 'xddd': 12059,\n",
       " '##fi': 9864,\n",
       " '##姨': 15064,\n",
       " '##tan': 10105,\n",
       " '##skip': 12724,\n",
       " '##躏': 19771,\n",
       " 'aphojoy': 9869,\n",
       " '##娟': 15083,\n",
       " '初': 1159,\n",
       " '斫': 3169,\n",
       " 'guestname': 13082,\n",
       " '牡': 4285,\n",
       " '##氡': 16761,\n",
       " '##盆': 17715,\n",
       " 'mwc': 12184,\n",
       " '##ves': 11084,\n",
       " '淑': 3902,\n",
       " '宓': 2132,\n",
       " '鎖': 7115,\n",
       " '拋': 2862,\n",
       " 'bigbang': 11122,\n",
       " '##楚': 16561,\n",
       " '##纯': 18340,\n",
       " '砥': 4783,\n",
       " '蘸': 5985,\n",
       " '呦': 1452,\n",
       " '膻': 5614,\n",
       " '##薬': 19017,\n",
       " '##諦': 19377,\n",
       " '##喫': 14661,\n",
       " 'aa': 9563,\n",
       " '##颼': 20653,\n",
       " '但': 852,\n",
       " 'root': 8859,\n",
       " '##ｼ': 21112,\n",
       " '##贍': 19616,\n",
       " 'history': 9939,\n",
       " '##毆': 16733,\n",
       " '##果': 16419,\n",
       " '癸': 4631,\n",
       " '岳': 2277,\n",
       " 'tc': 12858,\n",
       " '##骆': 20793,\n",
       " '##燙': 17300,\n",
       " '臻': 5638,\n",
       " '患': 2642,\n",
       " '##ましょう': 10759,\n",
       " '奄': 1935,\n",
       " '凛': 1123,\n",
       " '託': 6249,\n",
       " 'share': 8697,\n",
       " 'はお': 13232,\n",
       " '齡': 7972,\n",
       " '##繇': 18306,\n",
       " 'fans': 11821,\n",
       " '##惱': 15738,\n",
       " '##紳': 18227,\n",
       " '5m': 11483,\n",
       " 'ceo': 8371,\n",
       " '處': 5993,\n",
       " '##ゥ': 13681,\n",
       " 'ち': 552,\n",
       " '倭': 963,\n",
       " '##妘': 15031,\n",
       " '##暌': 16319,\n",
       " '哈': 1506,\n",
       " '苻': 5742,\n",
       " 'nvidia': 9503,\n",
       " '##研': 17834,\n",
       " '##狐': 17373,\n",
       " '绊': 5304,\n",
       " '旌': 3182,\n",
       " '##阉': 20386,\n",
       " 'nego': 11181,\n",
       " '##乔': 13787,\n",
       " '蒹': 5893,\n",
       " '##舱': 18722,\n",
       " 'install': 12461,\n",
       " '15058': 12347,\n",
       " 'ettoday': 9485,\n",
       " '##忖': 15618,\n",
       " '##燉': 17294,\n",
       " '##†': 13498,\n",
       " '##垠': 14859,\n",
       " '##桅': 16483,\n",
       " '鵡': 7867,\n",
       " '粧': 5115,\n",
       " '™': 362,\n",
       " 'ｗ': 8073,\n",
       " '##吼': 14489,\n",
       " '噌': 1680,\n",
       " '佔': 861,\n",
       " '##碾': 17884,\n",
       " '礁': 4842,\n",
       " '拮': 2888,\n",
       " '##キ': 10781,\n",
       " '##閘': 20338,\n",
       " '悵': 2652,\n",
       " '##銬': 20130,\n",
       " '##敦': 16199,\n",
       " '##缆': 18405,\n",
       " '穢': 4951,\n",
       " '##咦': 14541,\n",
       " '##剷': 14259,\n",
       " '##槌': 16597,\n",
       " '##163': 11631,\n",
       " '[unused44]': 44,\n",
       " '苔': 5726,\n",
       " 'costco': 10742,\n",
       " '詢': 6273,\n",
       " '##僭': 14072,\n",
       " '墊': 1865,\n",
       " '##鎔': 20171,\n",
       " '##酢': 20044,\n",
       " '##圜': 14815,\n",
       " 'amoled': 11307,\n",
       " '##胸': 18598,\n",
       " '##骰': 20814,\n",
       " '⋯⋯': 10169,\n",
       " '##wan': 9951,\n",
       " '篁': 5060,\n",
       " 'villa': 10806,\n",
       " 'evernote': 13138,\n",
       " '##詫': 19334,\n",
       " 'powered': 10103,\n",
       " '##賂': 19590,\n",
       " '##撃': 16105,\n",
       " '庁': 2409,\n",
       " '形': 2501,\n",
       " '鄢': 6970,\n",
       " '﹍': 7998,\n",
       " '##杼': 16407,\n",
       " 'ɡ': 199,\n",
       " '华': 1290,\n",
       " '芃': 5689,\n",
       " '##羹': 18473,\n",
       " '鉉': 7057,\n",
       " 'п': 247,\n",
       " '##y': 8179,\n",
       " '診': 6262,\n",
       " '##樊': 16614,\n",
       " '##麂': 20980,\n",
       " '##濠': 17147,\n",
       " '誕': 6293,\n",
       " '豈': 6488,\n",
       " '##淩': 16970,\n",
       " '116': 9070,\n",
       " '庹': 2436,\n",
       " '20g': 12793,\n",
       " '##vy': 11853,\n",
       " '##婧': 15102,\n",
       " '铰': 7210,\n",
       " '橙': 3581,\n",
       " '矗': 4755,\n",
       " 'gallery': 12595,\n",
       " '##瘋': 17654,\n",
       " '##嫖': 15126,\n",
       " '##层': 15288,\n",
       " '鸣': 7885,\n",
       " '##庭': 15488,\n",
       " '♀': 486,\n",
       " '歼': 3648,\n",
       " '573032185': 9970,\n",
       " '##嗬': 14694,\n",
       " '鲑': 7829,\n",
       " '伴': 845,\n",
       " '溱': 3986,\n",
       " ':': 131,\n",
       " '329': 11705,\n",
       " '##寅': 15222,\n",
       " '谟': 6467,\n",
       " '##ball': 11050,\n",
       " '##截': 15836,\n",
       " '退': 6842,\n",
       " '##れは': 9170,\n",
       " '惱': 2681,\n",
       " '##過': 19939,\n",
       " '##冏': 14144,\n",
       " '##life': 10359,\n",
       " 'before': 10735,\n",
       " '##畜': 17580,\n",
       " '##卵': 14374,\n",
       " '她': 1961,\n",
       " '##茭': 18814,\n",
       " 'ヨ': 634,\n",
       " '樸': 3571,\n",
       " '421': 12873,\n",
       " '##煲': 17273,\n",
       " '轩': 6759,\n",
       " '##蕨': 18995,\n",
       " '焘': 4188,\n",
       " '##碚': 17872,\n",
       " '##顏': 20599,\n",
       " 'emma': 12111,\n",
       " '##¤': 13349,\n",
       " '##捏': 15991,\n",
       " 'thai': 12967,\n",
       " '禽': 4896,\n",
       " '謐': 6337,\n",
       " '##融': 19141,\n",
       " '謨': 6344,\n",
       " '酩': 6990,\n",
       " '馀': 7663,\n",
       " '渡': 3941,\n",
       " '1903': 11295,\n",
       " 'very': 11785,\n",
       " '##诲': 19488,\n",
       " '##攻': 16179,\n",
       " '##虽': 19063,\n",
       " '抑': 2829,\n",
       " '230': 9111,\n",
       " '##殆': 16708,\n",
       " '椿': 3499,\n",
       " '╚': 440,\n",
       " '186': 9833,\n",
       " '壶': 1901,\n",
       " '鸭': 7890,\n",
       " 'さん': 10533,\n",
       " 'tee': 11330,\n",
       " '##时': 16255,\n",
       " '裁': 6161,\n",
       " '节': 5688,\n",
       " '##彌': 15550,\n",
       " '洹': 3832,\n",
       " 'crystal': 12070,\n",
       " '烯': 4179,\n",
       " '##腳': 18646,\n",
       " '鲢': 7833,\n",
       " '##轎': 19811,\n",
       " '##邳': 19995,\n",
       " '##鐸': 20194,\n",
       " '護': 6362,\n",
       " '叩': 1371,\n",
       " '##￥': 21123,\n",
       " '疲': 4558,\n",
       " '边': 6804,\n",
       " '##胄': 18575,\n",
       " '绛': 5316,\n",
       " '##称': 17974,\n",
       " '##颗': 20635,\n",
       " '##涼': 16950,\n",
       " '##鈞': 20104,\n",
       " '饋': 7637,\n",
       " '##dden': 13109,\n",
       " '祥': 4872,\n",
       " '醚': 7009,\n",
       " '##劃': 14262,\n",
       " 'ㄅ': 647,\n",
       " '銖': 7070,\n",
       " '##なたの': 11002,\n",
       " 'をこ': 10074,\n",
       " 'wei': 11875,\n",
       " 'ccd': 12882,\n",
       " '琬': 4428,\n",
       " 'nike': 8702,\n",
       " '##亳': 13837,\n",
       " '##饞': 20701,\n",
       " '##牌': 17334,\n",
       " '饕': 7642,\n",
       " '楚': 3504,\n",
       " 'icecat': 9336,\n",
       " '##滙': 17059,\n",
       " '算': 5050,\n",
       " '##涇': 16923,\n",
       " '翼': 5437,\n",
       " '痣': 4582,\n",
       " 'take': 10985,\n",
       " '##倆': 13998,\n",
       " '畫': 4529,\n",
       " 'alexander': 11733,\n",
       " '鲈': 7827,\n",
       " '##墮': 14933,\n",
       " '##裴': 19236,\n",
       " '溟': 3979,\n",
       " 'suv': 8540,\n",
       " '##n2': 12750,\n",
       " '##倾': 14024,\n",
       " '##秸': 17975,\n",
       " '##み': 9344,\n",
       " '勖': 1241,\n",
       " '##瘩': 17666,\n",
       " '##＇': 21076,\n",
       " '##蚕': 19071,\n",
       " '甩': 4501,\n",
       " '充': 1041,\n",
       " '嘯': 1669,\n",
       " 'imsean': 12087,\n",
       " '##犷': 17365,\n",
       " '##筱': 18093,\n",
       " '##菓': 18885,\n",
       " 'ib': 12487,\n",
       " '牢': 4286,\n",
       " '窥': 4976,\n",
       " '94': 8416,\n",
       " '##you': 12441,\n",
       " '館': 7631,\n",
       " '##・': 13703,\n",
       " '##ich': 11578,\n",
       " '##test': 11574,\n",
       " '##螞': 19142,\n",
       " '##膾': 18673,\n",
       " '腿': 5597,\n",
       " '##鲁': 20883,\n",
       " 'express': 9865,\n",
       " 'target': 11926,\n",
       " '汶': 3746,\n",
       " '齁': 7967,\n",
       " '谱': 6480,\n",
       " '##洲': 16885,\n",
       " '薦': 5956,\n",
       " '176': 10004,\n",
       " '##虫': 19058,\n",
       " '[unused63]': 63,\n",
       " 'eds': 12191,\n",
       " '##摯': 16098,\n",
       " '雏': 7422,\n",
       " 'koreanmall': 9895,\n",
       " '##壕': 14944,\n",
       " '睏': 4711,\n",
       " 'v9': 11894,\n",
       " '##仑': 13853,\n",
       " '##蹤': 19754,\n",
       " '糸': 5142,\n",
       " '羊': 5399,\n",
       " '##旁': 16235,\n",
       " '析': 3358,\n",
       " '倔': 949,\n",
       " '##諾': 19387,\n",
       " '##covery': 12548,\n",
       " '##贵': 19643,\n",
       " 'rights': 9615,\n",
       " '##しい': 10899,\n",
       " '甄': 4488,\n",
       " '##峥': 15343,\n",
       " '##淺': 16980,\n",
       " 'kuso': 10881,\n",
       " '気': 3700,\n",
       " '##见': 19281,\n",
       " '華': 5836,\n",
       " '##という': 12383,\n",
       " '##騁': 20753,\n",
       " '##core': 11921,\n",
       " 'beta': 9861,\n",
       " '##nny': 13239,\n",
       " '匣': 1271,\n",
       " '##瀬': 17172,\n",
       " '220': 8796,\n",
       " '##懋': 15805,\n",
       " '損': 3010,\n",
       " '##湫': 17021,\n",
       " 'ferragamo': 9992,\n",
       " '##懈': 15802,\n",
       " '##紙': 18215,\n",
       " '##翰': 18489,\n",
       " '摞': 3037,\n",
       " '##榭': 16588,\n",
       " '##吕': 14463,\n",
       " '桂': 3424,\n",
       " '氢': 3705,\n",
       " '敕': 3132,\n",
       " '赶': 6628,\n",
       " '限': 7361,\n",
       " '治': 3780,\n",
       " '##drive': 13115,\n",
       " '##※': 13508,\n",
       " 'fm': 9079,\n",
       " '##膠': 18665,\n",
       " 'v1': 9074,\n",
       " '菡': 5834,\n",
       " '裨': 6176,\n",
       " '钴': 7180,\n",
       " '##溢': 17037,\n",
       " '##埂': 14868,\n",
       " '##鼐': 21017,\n",
       " '铠': 7200,\n",
       " '淡': 3909,\n",
       " '氫': 3712,\n",
       " '##刍': 14207,\n",
       " '辗': 6786,\n",
       " '##覆': 19265,\n",
       " '趴': 6640,\n",
       " '当': 2496,\n",
       " 'www': 8173,\n",
       " '227': 10543,\n",
       " 'サ': 603,\n",
       " '贪': 6576,\n",
       " '237': 10775,\n",
       " '242': 11056,\n",
       " '忘': 2563,\n",
       " '##bb': 10214,\n",
       " '##曲': 16346,\n",
       " '尘': 2212,\n",
       " '##農': 19860,\n",
       " '##ては': 9809,\n",
       " '鏞': 7126,\n",
       " 'ς': 225,\n",
       " '悍': 2636,\n",
       " 'ｯ': 8086,\n",
       " 'was': 9947,\n",
       " '卅': 1284,\n",
       " '郜': 6949,\n",
       " 'м': 244,\n",
       " '﹂': 7997,\n",
       " '##竅': 18043,\n",
       " '##釵': 20097,\n",
       " '巢': 2338,\n",
       " '１２': 10351,\n",
       " '##cast': 13020,\n",
       " '##厲': 14398,\n",
       " '##方': 16232,\n",
       " '##牍': 17335,\n",
       " '诅': 6398,\n",
       " '##撕': 16113,\n",
       " '##秣': 17967,\n",
       " 'step3': 9434,\n",
       " '聖': 5469,\n",
       " '散': 3141,\n",
       " '##泻': 16868,\n",
       " 'etnews': 12870,\n",
       " '檗': 3594,\n",
       " '厩': 1338,\n",
       " '舍': 5650,\n",
       " '##宰': 15210,\n",
       " '籲': 5100,\n",
       " '##line': 8762,\n",
       " ...}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab\n",
    "# 带\"##\"的是把一个完整的词拆成多个子词，\n",
    "# 从而缩小词表，很多词使用几个子词来组成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21128"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 索引转换: `convert_tokens_to_ids`/`convert_ids_to_tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将词序列转换为id序列\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['弱', '小', '的', '我', '也', '有', '大', '梦', '想', '!']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将id序列转换为token序列\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'弱 小 的 我 也 有 大 梦 想!'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将token序列转换为string\n",
    "str_sen = tokenizer.convert_tokens_to_string(tokens)\n",
    "str_sen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  字符串和id序列的转换: `encode`/`decode`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106, 102]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将字符串转换为id序列，又称之为编码\n",
    "ids = tokenizer.encode(sen, add_special_tokens=True)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(sen, add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 弱 小 的 我 也 有 大 梦 想! [SEP]'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将id序列转换为字符串，又称之为解码\n",
    "str_sen = tokenizer.decode(ids, skip_special_tokens=False)\n",
    "str_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'弱 小 的 我 也 有 大 梦 想!'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids, skip_special_tokens=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 填充与截断: `padding`/`truncation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106, 102, 0, 0, 0]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 填充\n",
    "ids = tokenizer.encode(sen, padding=\"max_length\", max_length=15)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2483, 2207, 4638, 102]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 截断\n",
    "ids = tokenizer.encode(sen, max_length=5, truncation=True)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106, 102]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2483, 2207, 4638, 2769, 102, 2483, 2207, 4638, 2769, 738, 102]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 两个都截断\n",
    "tokenizer.encode([sen, sen], max_length=12, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 2483,\n",
       " 2207,\n",
       " 4638,\n",
       " 2769,\n",
       " 738,\n",
       " 3300,\n",
       " 1920,\n",
       " 102,\n",
       " 2483,\n",
       " 2207,\n",
       " 4638,\n",
       " 2769,\n",
       " 738,\n",
       " 3300,\n",
       " 1920,\n",
       " 3457,\n",
       " 2682,\n",
       " 106,\n",
       " 102]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 只截断了第一个\n",
    "tokenizer.encode([sen, sen], max_length=20, truncation=\"only_first\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 其他输入部分: `attention_mask`/`token_type_ids`\n",
    "\n",
    "* `attention_mask`：标记哪些部分的token是有意义的，哪些部分是padding的\n",
    "\n",
    "* `token_type_ids`：标记哪些部分的token属于第一个句子，哪些部分的token属于第二个句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106, 102, 0, 0, 0]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = tokenizer.encode(sen, padding=\"max_length\", max_length=15)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106, 102, 0, 0, 0],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attention_mask:\n",
    "# 0表示padding，1表示非padding\n",
    "# token_type_ids:\n",
    "# 0表示第一个句子，1表示第二个句子\n",
    "# 对于单句任务，token_type_ids全为0\n",
    "# 对于双句任务，token_type_ids的前半部分为0，后半部分为1\n",
    "attention_mask = [1 if idx != tokenizer.pad_token_id else 0 for idx in ids]\n",
    "token_type_ids = [0] * len(ids)\n",
    "ids, attention_mask, token_type_ids"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 快速调用方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106, 102, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer.encode_plus(sen, padding=\"max_length\", max_length=15)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 106, 102, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(sen, padding=\"max_length\", max_length=15)\n",
    "inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理batch数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 102], [101, 3300, 3457, 2682, 6443, 6963, 749, 679, 6629, 102], [101, 6841, 6852, 3457, 2682, 4638, 2552, 8024, 3683, 3457, 2682, 3315, 6716, 8024, 3291, 1377, 6586, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sens = [\"弱小的我也有大梦想\", \"有梦想谁都了不起\", \"追逐梦想的心，比梦想本身，更可贵\"]\n",
    "res = tokenizer(sens)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.9 ms, sys: 0 ns, total: 30.9 ms\n",
      "Wall time: 30.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 单条循环处理\n",
    "for i in range(1000):\n",
    "    tokenizer(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.1 ms, sys: 0 ns, total: 36.1 ms\n",
      "Wall time: 6.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 处理batch数据\n",
    "res = tokenizer([sen] * 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='./roberta_tokenizer/', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast / Slow Tokenizer\n",
    "Fast Tokenizer：\n",
    "\n",
    "* 基于Rust实现，速度快\n",
    "\n",
    "* offsets_mapping、word_ids\n",
    "\n",
    "Slow Tokenizer：\n",
    "\n",
    "* 基于python实现，速度慢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = \"弱小的我也有大Dreaming!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='uer/roberta-base-finetuned-dianping-chinese', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"uer/roberta-base-finetuned-dianping-chinese\"\n",
    ")\n",
    "fast_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='uer/roberta-base-finetuned-dianping-chinese', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slow_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"uer/roberta-base-finetuned-dianping-chinese\", use_fast=False\n",
    ")\n",
    "slow_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 309 ms, sys: 2.74 ms, total: 312 ms\n",
      "Wall time: 312 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 单条循环处理\n",
    "for i in range(10000):\n",
    "    fast_tokenizer(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 819 ms, sys: 1.92 ms, total: 821 ms\n",
      "Wall time: 823 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 单条循环处理\n",
    "for i in range(10000):\n",
    "    slow_tokenizer(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 323 ms, sys: 15.1 ms, total: 338 ms\n",
      "Wall time: 88.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 处理batch数据\n",
    "res = fast_tokenizer([sen] * 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 672 ms, sys: 0 ns, total: 672 ms\n",
      "Wall time: 672 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 处理batch数据\n",
    "res = slow_tokenizer([sen] * 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 10252, 8221, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 12), (12, 15), (15, 16), (0, 0)]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sen = \"弱小的我也有大Dreaming!\"\n",
    "# return_offsets_mapping: dreaming被分为dream和ing\n",
    "# word_ids指的是对应原句的哪个词，dream对应第7词，ing也是\n",
    "# offsets_mapping指的是每个token对应的index，(7,12)指的是dream，(12,15)指的是ing，(15,16)为!\n",
    "inputs = fast_tokenizer(sen, return_offsets_mapping=True)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "return_offset_mapping is not available when using Python tokenizers. To use this feature, change your tokenizer to one deriving from transformers.PreTrainedTokenizerFast. More information on available tokenizers at https://github.com/huggingface/transformers/pull/2674",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mslow_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/App/miniconda3/envs/transformers/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2945\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2943\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2944\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2945\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2946\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2947\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/App/miniconda3/envs/transformers/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3053\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3032\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m   3033\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   3034\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3050\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3051\u001b[0m     )\n\u001b[1;32m   3052\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3053\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3056\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3057\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3058\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3059\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3060\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3061\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3062\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3063\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3064\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3065\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3066\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3069\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3070\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3071\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3072\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3073\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/App/miniconda3/envs/transformers/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3127\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3117\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3118\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3119\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3120\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3124\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3125\u001b[0m )\n\u001b[0;32m-> 3127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3130\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3145\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msplit_special_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3146\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3147\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/App/miniconda3/envs/transformers/lib/python3.10/site-packages/transformers/tokenization_utils.py:780\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    774\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    775\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not valid. Should be a string, a list/tuple of strings or a list/tuple of\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    776\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m integers.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    777\u001b[0m             )\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_offsets_mapping:\n\u001b[0;32m--> 780\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    781\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    782\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers.PreTrainedTokenizerFast. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMore information on available tokenizers at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    786\u001b[0m     )\n\u001b[1;32m    788\u001b[0m first_ids \u001b[38;5;241m=\u001b[39m get_input_ids(text)\n\u001b[1;32m    789\u001b[0m second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(text_pair) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: return_offset_mapping is not available when using Python tokenizers. To use this feature, change your tokenizer to one deriving from transformers.PreTrainedTokenizerFast. More information on available tokenizers at https://github.com/huggingface/transformers/pull/2674"
     ]
    }
   ],
   "source": [
    "inputs = slow_tokenizer(sen, return_offsets_mapping=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特殊Tokenizer的加载\n",
    "\n",
    "不同模型对应的分词器的效果也不同，有的会在结尾加上`<\\s>`，有的会在开头加`<CLS>`在结尾加`<EOS>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers_modules.Skywork.Skywork-13B-base.bc35915066fbbf15b77a1a4a74e9b574ab167816.tokenization_skywork.SkyworkTokenizer'>. This means that tokens that come after special tokens will not be properly handled. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SkyworkTokenizer(name_or_path='Skywork/Skywork-13B-base', vocab_size=65519, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 新版本的transformers（>4.34），加载 THUDM/chatglm 会报错，因此这里替换为了天宫的模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Skywork/Skywork-13B-base\", trust_remote_code=True\n",
    ")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('skywork_tokenizer/tokenizer_config.json',\n",
       " 'skywork_tokenizer/special_tokens_map.json',\n",
       " 'skywork_tokenizer/tokenizer.model',\n",
       " 'skywork_tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"skywork_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"skywork_tokenizer\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>弱小的我也有大Dreaming!'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(sen))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
